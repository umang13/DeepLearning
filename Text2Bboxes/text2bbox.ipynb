{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.48s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import sys\n",
    "from model_new import Text2BBoxesModel\n",
    "from dataset import get_data_loader_and_cats\n",
    "\n",
    "\n",
    "dataType='val2017'\n",
    "dataDir = './Datasets/coco/images/{}/'.format(dataType)\n",
    "annFile_Detection ='./Datasets/coco/annotations/instances_{}.json'.format(dataType)\n",
    "annFile_Caption ='./Datasets/coco/annotations/captions_{}.json'.format(dataType)\n",
    "batch_size = 256\n",
    "coco = COCO(annFile_Detection)\n",
    "coco_captions = COCO(annFile_Caption)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embed_size = 768\n",
    "epochs = 30\n",
    "learning_rate = 1e-3\n",
    "num_mixtures = 5\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "ckpt_file_path = \"./outputs/checkpoint_latest.ckpt\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.62s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "dataloader, total_categories = get_data_loader_and_cats(annFile_Caption, annFile_Detection, batch_size, True)\n",
    "start = time.time()\n",
    "num_classes = total_categories[max(total_categories, key=total_categories.get)] + 1\n",
    "model = Text2BBoxesModel(embed_size, total_categories, batch_size, device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, betas=(beta1, beta2))\n",
    "\n",
    "# mdn_xy_model = MDN(2, num_mixtures)\n",
    "# mdn_wh_model = MDN(2, num_mixtures)\n",
    "# if(os.path.exists(ckpt_file_path)) :\n",
    "#     checkpoint = torch.load(ckpt_file_path, map_location=device)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#     print(\"Model loaded with training up to {} epochs and {} minibatches\".format(checkpoint['epoch'], checkpoint['count']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/umangsharma/Documents/Projects/DeepLearning/Text2Bboxes/MDN.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(self.pi_layer(input))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n",
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n",
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n",
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n",
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n",
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n",
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n",
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n",
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n",
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n",
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n",
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n",
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n",
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n",
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n",
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n",
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n",
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n",
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n",
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n",
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n",
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n",
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n",
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n",
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n",
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n",
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n",
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n",
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n",
      "====> torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5]) torch.Size([256, 5])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-24f7232552f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss after epoch {} : {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "for epoch in range(epochs) : \n",
    "    count = 0\n",
    "    for _, captions_mini_batch, bboxes, categories, lengths in dataloader :\n",
    "        captions_mini_batch = captions_mini_batch.to(device)\n",
    "        bboxes = bboxes.to(device)\n",
    "        categories = categories.to(device)\n",
    "        loss_what = 0\n",
    "        loss_where = 0\n",
    "        optimizer.zero_grad()\n",
    "        max_length = max(lengths)\n",
    "        lengths = torch.Tensor(lengths)\n",
    "        outputs = model(captions_mini_batch, max_length)\n",
    "        \n",
    "\n",
    "        categories = categories.permute(1, 0)\n",
    "        seqlen = categories.shape[0] \n",
    "        pred_labels = outputs[:,:,:-4]\n",
    "        pred_bboxes_xy = outputs[:,:,-4:-2]\n",
    "        pred_bboxes_wh = outputs[:,:,-2:]\n",
    "        \n",
    "#         theta_xy = mdn_xy_model(pred_bboxes_xy)\n",
    "#         theta_wh = mdn_wh_model(pred_bboxes_wh)\n",
    "#         print(theta_xy.shape, theta_wh.shape)\n",
    "        for i in range(0,seqlen) :\n",
    "            loss_what += F.cross_entropy(pred_labels[i, :, :], categories[i, :].long())\n",
    "        loss_what /= seqlen\n",
    "        loss_what.backward()\n",
    "        optimizer.step()\n",
    "        count += 1            \n",
    "        if(count % 100 == 0) :\n",
    "            checkpoint = {'model_state_dict': model.state_dict(),\n",
    "                          'optimizer_state_dict' : optimizer.state_dict(),\n",
    "                          'epoch'  : epoch,\n",
    "                          'count'  : count}\n",
    "            torch.save(checkpoint, ckpt_file_path)\n",
    "            print(\"After processing {} minibatches in epoch {} , loss is {}\".format(count, epoch, loss))\n",
    "        if(epoch == 0) :\n",
    "            break\n",
    "\n",
    "\n",
    "    print(\"Loss after epoch {} : {}\".format(epoch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on validation sets\n",
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.38s)\n",
      "creating index...\n",
      "index created!\n",
      "Loss on validaton set : 0.5318634510040283\n",
      "Predicted labels shape : torch.Size([47, 256]), Ground truth labels shape torch.Size([47, 256])\n",
      "Text input : An old fashioned kitchen with the light on. \n",
      "Predicted labels:\n",
      "['start', 'refrigerator', 'oven', 'oven', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
      "Ground truth labels:\n",
      "['start', 'refrigerator', 'chair', 'dining table', 'bowl', 'bowl', 'sink', 'cup', 'spoon', 'chair', 'banana', 'cup', 'end', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
      "Text input : A woman in a blue jacket is skiing in front of a house.\n",
      "Predicted labels:\n",
      "['start', 'person', 'skis', 'end', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
      "Ground truth labels:\n",
      "['start', 'person', 'snowboard', 'end', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
      "Text input : Several people playing tennis in a school gymnasium.\n",
      "Predicted labels:\n",
      "['start', 'sports ball', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
      "Ground truth labels:\n",
      "['start', 'tennis racket', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'tennis racket', 'tennis racket', 'person', 'person', 'end', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
      "Text input : A black cat sitting under a park bench.\n",
      "Predicted labels:\n",
      "['start', 'cat', 'bench', 'end', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
      "Ground truth labels:\n",
      "['start', 'cat', 'bench', 'end', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
      "Text input : Pumpkins are made with different faces and one has flowers. \n",
      "Predicted labels:\n",
      "['start', 'vase', 'vase', 'end', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
      "Ground truth labels:\n",
      "['start', 'vase', 'end', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
      "Text input : A cat is sitting on a desk next to a computer.\n",
      "Predicted labels:\n",
      "['start', 'cat', 'laptop', 'laptop', 'book', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
      "Ground truth labels:\n",
      "['start', 'tv', 'cat', 'laptop', 'keyboard', 'book', 'book', 'book', 'cell phone', 'book', 'end', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
      "Text input : Woman carrying cake near man holding baby at outdoor celebration.\n",
      "Predicted labels:\n",
      "['start', 'person', 'person', 'person', 'person', 'person', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
      "Ground truth labels:\n",
      "['start', 'chair', 'chair', 'person', 'person', 'person', 'person', 'cake', 'person', 'dining table', 'end', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
      "Text input : A laptop, phone, keys and other accessories sitting on a table.\n",
      "Predicted labels:\n",
      "['start', 'cell phone', 'cell phone', 'laptop', 'end', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
      "Ground truth labels:\n",
      "['start', 'cell phone', 'laptop', 'mouse', 'book', 'book', 'remote', 'end', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
      "Text input : A sink with a cabinet on legs in a bathroom.\n",
      "Predicted labels:\n",
      "['start', 'sink', 'sink', 'end', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
      "Ground truth labels:\n",
      "['start', 'sink', 'end', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
      "Text input : A painting of flowers in vase setting on a table with two chairs.\n",
      "Predicted labels:\n",
      "['start', 'vase', 'vase', 'end', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
      "Ground truth labels:\n",
      "['start', 'potted plant', 'chair', 'chair', 'end', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictions on validation sets\")\n",
    "with torch.no_grad() :\n",
    "        dataType = 'val2017'\n",
    "        dataDir = './Datasets/coco/images/{}/'.format(dataType)\n",
    "        annFile_Detection ='./Datasets/coco/annotations/instances_{}.json'.format(dataType)\n",
    "        annFile_Caption ='./Datasets/coco/annotations/captions_{}.json'.format(dataType)\n",
    "        val_dataloader, total_categories = get_data_loader_and_cats(annFile_Caption, annFile_Detection, batch_size, True)\n",
    "        val_loss = 0\n",
    "        for caption_txt, caption, bboxes, categories, lengths in val_dataloader :\n",
    "            caption = caption.to(device)\n",
    "            bboxes = bboxes.to(device)\n",
    "            categories = categories.to(device)\n",
    "            max_length = max(lengths)\n",
    "            pred_labels = model(caption, max_length)\n",
    "            vals, idx = torch.max(pred_labels, 2)\n",
    "            categories = categories.permute(1, 0)\n",
    "            seqlen = categories.shape[0] \n",
    "            for i in range(0,seqlen) :\n",
    "                val_loss += F.cross_entropy(pred_labels[i, :, :], categories[i, :].long())\n",
    "            val_loss /= seqlen\n",
    "            break\n",
    "        key_list = list(total_categories.keys()) \n",
    "        val_list = list(total_categories.values()) \n",
    "        print(\"Loss on validaton set : {}\".format(val_loss))\n",
    "        print(\"Predicted labels shape : {}, Ground truth labels shape {}\".format(idx.shape, categories.shape))\n",
    "        # Print a sample of 10 captions from the val set\n",
    "        for i in range(10) :\n",
    "            print(\"Text input : {}\".format(caption_txt[i]))            \n",
    "            print(\"Predicted labels:\")\n",
    "            print([key_list[val_list.index(j)] for j in idx[:,i].tolist()])\n",
    "            print(\"Ground truth labels:\")\n",
    "            print([key_list[val_list.index(j)] for j in categories[:,i].tolist()])\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.36s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0bc42907077a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mannFile_Caption\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'./Datasets/coco/annotations/captions_{}.json'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_categories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_loader_and_cats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannFile_Caption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannFile_Detection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_dataloader\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "dataType = 'val2017'\n",
    "dataDir = './Datasets/coco/images/{}/'.format(dataType)\n",
    "annFile_Detection ='./Datasets/coco/annotations/instances_{}.json'.format(dataType)\n",
    "annFile_Caption ='./Datasets/coco/annotations/captions_{}.json'.format(dataType)\n",
    "val_dataloader, total_categories = get_data_loader_and_cats(annFile_Caption, annFile_Detection, batch_size, True)\n",
    "for caption, bboxes, categories, lengths in val_dataloader :\n",
    "    max_length = max(lengths)\n",
    "    with torch.no_grad() :\n",
    "        pred_labels = model(caption, max_length)\n",
    "        vals, idx = torch.max(pred_labels, 2)\n",
    "        categories = categories.permute(1, 0)\n",
    "        seqlen = categories.shape[0] \n",
    "        val_loss = 0\n",
    "        print(pred_labels.shape, categories.shape)\n",
    "        for i in range(0,seqlen) :\n",
    "            val_loss += F.cross_entropy(pred_labels[i, :, :], categories[i, :].long())\n",
    "        val_loss /= seqlen\n",
    "        print(\"Loss on validaton set : {}\".format(val_loss))\n",
    "        print(\"Predicted labels shape : {}, Ground truth labels shape {}\".format(idx.shape, categories.shape))\n",
    "        print(\"Predicted labels {}, Ground truth labels {}\".format(total_categories[idx[:,0]], total_categories[categories[:,0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1])\n"
     ]
    }
   ],
   "source": [
    "start_tensor = self.index_to_one_hot(self.SOS)\n",
    "label = start_tensor.repeat(self.batch_size,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 3: out of range at /Users/distiller/project/conda/conda-bld/pytorch_1565272526878/work/aten/src/TH/generic/THTensor.cpp:370",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-93666fb6e581>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbbox_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 3: out of range at /Users/distiller/project/conda/conda-bld/pytorch_1565272526878/work/aten/src/TH/generic/THTensor.cpp:370"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
